% --- Safe guards ---
\makeatletter
\@ifundefined{remark}{\theoremstyle{remark}\newtheorem{remark}{Remark}}{}
\@ifundefined{lemma}{\theoremstyle{plain}\newtheorem{lemma}{Lemma}}{}
\@ifundefined{theorem}{\theoremstyle{plain}\newtheorem{theorem}{Theorem}}{}
\@ifundefined{theorem*}{\newtheorem*{theorem*}{Theorem}}{}
\makeatother
\providecommand{\fitcol}[1]{#1}
% ----------------------------------

\section*{Lyapunov / ISS Stability of the Adaptive Law $R_S \propto \sigma_a \tau^3$}
\label{sec:lyap-iss}

We establish input–to–state stability (ISS) of the estimation error under the adaptive
pseudo-measurement weighting
\[
R_S(\theta)=\mathrm{diag}(r_{Sx},r_{Sy},r_{Sz}),\qquad
r_{Si}=k_i\,\sigma_a\,\tau^3,
\]
where $\theta=(\sigma_a,\tau)$ are adapted online and $k_i>0$ are fixed anisotropy
coefficients (possibly $k_x,k_y\ll k_z$).
We analyze the extended Q–MEKF linearized after attitude lock, focusing on the translational OU chain
\[
\begin{aligned}
\dot v&=a_w,\qquad \dot p=v,\qquad \dot S=p,\qquad\\
\dot a_w&=-\tfrac{1}{\tau}a_w+\sqrt{q_c}\,w,\quad q_c=2\sigma_a^2/\tau .
\end{aligned}
\]

\subsection*{Assumptions}
\begin{itemize}[before=\small,itemsep=2pt,topsep=2pt,
  labelsep=0.6em,labelwidth=2.2em,labelindent=0pt,leftmargin=!]

\item[A1] \textbf{Bounded sampling and small linearization error.}
Sampling $h\in(0,\bar h]$ such that $\Phi(h,\tau)$ matches the analytic form; the
linearization error is Lipschitz on the compact set considered.

\item[A2] \textbf{Noise moments (SDE sense).}
Sensor noises have finite second moments (e.g., sub-Gaussian). The OU driver $w$ is standard white noise
in the SDE sense; discrete innovations are zero-mean with finite covariance.

\item[A2$'$] \textbf{Disturbance decomposition (may depend on $e_k$).}
There exist $\bar d\ge0$ and $L_d\ge0$ such that
\[
\|d_k\|\;\le\;\bar d + L_d\,\|e_k\|\qquad\text{for all }k.
\]
(Exogenous case: $L_d=0$, $\bar d=\sup_k\|d_k\|$.)

\item[A3] \textbf{Parameter compactness via projection.}
\[
\Theta=\{(\sigma_a,\tau):\underline\sigma\le\sigma_a\le\overline\sigma,\;
\underline\tau\le\tau\le\overline\tau\},
\]
enforced by a projection $\Pi_\Theta(\cdot)$.

\item[A4] \textbf{CQLF for the frozen closed loop.}
Let $A(\theta):=\Phi(\theta)-K(\theta)C\,\Phi(\theta)$. There exist $P\succ0$ and $\mu\in(0,1)$ such that
\begin{equation}
\label{eq:cqlf}
A(\theta)^\top P\,A(\theta)\;\preceq\;(1-\mu)\,P\qquad \forall\,\theta\in\Theta .
\end{equation}
Denote the condition number $\kappa:=\lambda_{\max}(P)/\lambda_{\min}(P)$.
\end{itemize}

Let $e:=x-x^\star$ be the stacked estimation error for the linear translational subsystem
(including $a_w$), with $x^\star$ the true OU-driven state.

\begin{lemma}[One-step Lyapunov inequality with explicit constants]\label{lem:onestep}
Under \textup{A1}, \textup{A2$'$}, and \textup{A4}, with $V(e)=e^\top P e$,
for any $\varepsilon\in(0,\mu)$ the update $e_{k+1}=A(\theta_k)e_k+d_k$ satisfies
\begin{equation}\label{eq:Vdec-sharp}
V(e_{k+1})
\;\le\;
\Big(1-\mu+\varepsilon + 2\kappa L_d^2\big[\,1+\tfrac{1-\mu}{\varepsilon}\,\big]\Big)\,V(e_k)
\;+\; 2\lambda_{\max}(P)\Big[\,1+\tfrac{1-\mu}{\varepsilon}\,\Big]\bar d^{\,2}.
\end{equation}
\end{lemma}

\begin{proof}
Write
\[
V(e_{k+1}) = e_k^\top A_k^\top P A_k e_k + 2 e_k^\top A_k^\top P d_k + d_k^\top P d_k,
\]
with $A_k:=A(\theta_k)$. By \eqref{eq:cqlf}, the first term $\le (1-\mu)V(e_k)$.
For the cross term, Young’s inequality gives, for any $\varepsilon\in(0,\mu)$,
\[
2 e_k^\top A_k^\top P d_k
\;\le\; \varepsilon\,V(e_k)
+ \frac{1}{\varepsilon}\,d_k^\top P A_k P^{-1} A_k^\top P d_k.
\]
Since $A_k^\top P A_k \preceq (1-\mu)P$, we have
$P^{-\!1/2}A_k^\top P A_k P^{-\!1/2}\preceq (1-\mu)I$, hence
$P A_k P^{-1} A_k^\top P \preceq (1-\mu)\,P$ and
\[
2 e_k^\top A_k^\top P d_k \;\le\; \varepsilon\,V(e_k) + \frac{1-\mu}{\varepsilon}\,d_k^\top P d_k.
\]
Thus
\[
V(e_{k+1}) \le (1-\mu+\varepsilon)\,V(e_k) + \Big(1+\tfrac{1-\mu}{\varepsilon}\Big)\,d_k^\top P d_k.
\]
Using A2$'$, $\|d_k\|^2 \le 2L_d^2\|e_k\|^2 + 2\bar d^{\,2}
\le \tfrac{2L_d^2}{\lambda_{\min}(P)}V(e_k) + 2\bar d^{\,2}$, and $d_k^\top P d_k\le \lambda_{\max}(P)\|d_k\|^2$,
we obtain \eqref{eq:Vdec-sharp}.
\end{proof}

\begin{theorem*}[ISS with small-gain condition; exogenous case as corollary]\label{thm:iss}
Assume \textup{A1}, \textup{A2$'$}, and \textup{A4}. If there exists $\varepsilon\in(0,\mu)$ such that
\begin{equation}\label{eq:small-gain}
\boxed{\quad
2\kappa L_d^2\Big(1+\frac{1-\mu}{\varepsilon}\Big) \;<\; \mu - \varepsilon
\quad}
\end{equation}
(the inequality holding uniformly on $\Theta$), then the error satisfies the ISS estimate
\[
\|e_k\|\;\le\; \beta(\|e_0\|,k) \;+\; \gamma(\bar d),
\]
for some class-$\mathcal{KL}$ function $\beta$ and class-$\mathcal{K}$ function $\gamma$ depending on $(\mu,\kappa,P)$.
In particular, if $d_k$ is exogenous ($L_d=0$), \eqref{eq:small-gain} reduces to choosing any $\varepsilon\in(0,\mu)$,
and ISS holds with no further restriction.
\end{theorem*}

\begin{proof}
Under \eqref{eq:small-gain}, the coefficient of $V(e_k)$ in \eqref{eq:Vdec-sharp} is strictly $<1$.
Iterating \eqref{eq:Vdec-sharp} yields geometric decay plus an ultimate bound proportional to $\bar d^{\,2}$.
Standard discrete-time ISS results then imply the stated bound.
\end{proof}

\begin{remark}[Closed-form sufficient test]
The left-hand side in \eqref{eq:small-gain} is minimized at
\(\varepsilon^\star = \sqrt{(1-\mu)\,2\kappa}\,L_d\).
Substituting $\varepsilon^\star$ gives the explicit condition
\[
\boxed{\qquad
2\sqrt{2\kappa(1-\mu)}\,L_d \;+\; 2\kappa\,L_d^2 \;<\; \mu.
\qquad}
\]
This reveals how tighter damping (larger $\mu$), better conditioning ($\kappa\!\downarrow$), or smaller residue $L_d$
ensure ISS. In the exogenous case ($L_d=0$) the inequality is trivially satisfied.
\end{remark}

\paragraph{Interpretation.}
A common quadratic Lyapunov function (A4) gives a uniform contraction for the frozen loop.
If $d_k$ is purely exogenous, we recover
\(
V(e_{k+1}) \le (1-\mu+\varepsilon)\,V(e_k) + 2\lambda_{\max}(P)[1+(1-\mu)/\varepsilon]\,\bar d^{\,2}
\),
hence ISS. If $d_k$ contains state-dependent residue, the small-gain bound above makes the contraction explicit.

\subsection*{How to verify the CQLF (A4) in practice}
Discretize a grid of $(\sigma_a,\tau)$ in $\Theta$ and solve
\[
A(\theta_i)^\top P A(\theta_i) \preceq (1-\mu)P,\qquad P\succ0
\]
for a common $P,\mu$ using an LMI solver (e.g.\ \texttt{feasp}, \texttt{CVX}, or \texttt{YALMIP}/\texttt{MOSEK}).
If infeasible: (i) narrow $\Theta$; (ii) raise $R_S$ floors; (iii) reduce the linearization window; or
(iv) allow a parameter-dependent $P(\theta)$ (LPV).

\subsection*{Implementation checklist}
\begin{enumerate}\itemsep2pt
\item Projection: $\hat\sigma_a\!\in[\underline\sigma,\overline\sigma]$, $\hat\tau\!\in[\underline\tau,\overline\tau]$ (A3).
\item Noise floors: keep $Q\succeq q_{\min}I$, $R,R_S\succeq r_{\min}I$ (improves $\mu$ and $\kappa$).
\item Timescale separation: update $(\hat\sigma_a,\hat\tau)$ slower than $(v,p,S)$.
\item Anisotropy: $R_S=\mathrm{diag}(k_x,k_y,k_z)\,\sigma_a\tau^3$ with $k_x,k_y\!\ll\!k_z$ when appropriate.
\item (Optional) Rate limit $\|\theta_{k+1}-\theta_k\|\!\le\!\rho$ to improve transients; not required for stability here.
\end{enumerate}

\section*{Why $R_S \propto \sigma_a \tau^3$ (Dimensional and Spectral Justification)}
\label{app:rs-scaling}

\paragraph{Notation and units.}
$a_w$ is OU with std.\ $\sigma_a$ and correlation $\tau$,
\[
\fitcol{
\dot a_w = -\tfrac{1}{\tau}a_w + \sqrt{q_c}\,w, \qquad q_c=\tfrac{2\sigma_a^2}{\tau}.
}
\]
The chain $\dot v=a_w,\;\dot p=v,\;\dot S=p$ implies $S$ is the triple time integral of $a_w$.
$R_S$ is a standard deviation (so $R_S^2$ appears in covariance).

\begin{remark}[Back-of-the-envelope scaling]\label{rem:dim}
Over one correlation time $\tau$, each integration contributes a factor $\sim\tau$ in magnitude:
$v=\mathcal O(\sigma_a\tau)$, $p=\mathcal O(\sigma_a\tau^2)$, $S=\mathcal O(\sigma_a\tau^3)$.
This heuristic matches the spectral calculation below.
\end{remark}

\paragraph{Spectral justification with a low-frequency cutoff.}
With $S_a(\omega)=\tfrac{2\sigma_a^2\tau}{1+\omega^2\tau^2}$ and $H_S(j\omega)=1/(j\omega)^3$,
the variance of $S$ without a low-frequency cutoff diverges. Using a physically motivated cutoff
$\omega_{\min}\approx 1/\tau$ (or detrending over windows of length $\sim\tau$), we obtain
\[
\fitcol{
\mathrm{Var}[S]
= \frac{1}{2\pi}\!\int_{\omega_{\min}}^\infty \! S_a(\omega)\,\big|H_S(j\omega)\big|^2\,d\omega
\approx \frac{1}{2\pi}\!\int_{1/\tau}^\infty \frac{2\sigma_a^2\tau}{1+\omega^2\tau^2}\,\frac{1}{\omega^6}\,d\omega.
}
\]
With the substitution $u = \omega\tau$, $d\omega = du/\tau$, this becomes
\[
\fitcol{
\mathrm{Var}[S] \approx \frac{\sigma_a^2\tau^6}{\pi}\int_{1}^{\infty}\frac{du}{(1+u^2)\,u^6}.
}
\]
Symbolically,
\[
\int_{1}^{\infty}\frac{du}{(1+u^2)\,u^6} = \frac{13}{15} - \frac{\pi}{4}
\quad\Rightarrow\quad
\]
\[
\mathrm{std}(S)\approx 0.16084\,\sigma_a\,\tau^3.
\]
Thus choosing
\[
R_S = k_i\,\sigma_a\,\tau^3
\quad\Longleftrightarrow\quad
R_S^2 = k_i^2\,\sigma_a^2\,\tau^6
\]
makes the innovation SNR roughly invariant to sea-state energy and correlation length, consistent with
the OU chain’s discrete $Q_d$ scaling.
