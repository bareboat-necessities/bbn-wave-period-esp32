% --- Safe guards (put anywhere before this section, e.g., right above it) ---
\makeatletter
% If you're not already loading amsthm in the preamble, make sure you do:
% \usepackage{amsthm}
\@ifundefined{remark}{\theoremstyle{remark}\newtheorem{remark}{Remark}}{}
\@ifundefined{lemma}{\theoremstyle{plain}\newtheorem{lemma}{Lemma}}{}
\@ifundefined{theorem}{\theoremstyle{plain}\newtheorem{theorem}{Theorem}}{}
% Unnumbered theorem, used as theorem*
\@ifundefined{theorem*}{\newtheorem*{theorem*}{Theorem}}{}
\makeatother
% Provide a harmless default for \fitcol if not already defined
\providecommand{\fitcol}[1]{#1}
% ---------------------------------------------------------------------------

\section*{Lyapunov / ISS Stability of the Adaptive Law $R_S \propto \sigma_a \tau^3$}
\label{sec:lyap-iss}

We establish input–to–state stability (ISS) of the estimation error under the adaptive
pseudo-measurement weighting
\[
R_S(\theta)=\mathrm{diag}(r_{Sx},r_{Sy},r_{Sz}),\qquad
r_{Si}=k_i\,\sigma_a\,\tau^3,
\]
where $\theta=(\sigma_a,\tau)$ are adapted online and $k_i>0$ are fixed anisotropy
coefficients (possibly $k_x,k_y\ll k_z$).
We analyze the extended Q–MEKF linearized after attitude lock, focusing on the translational OU chain
\[
\begin{align}
\dot v&=a_w,\qquad \dot p=v,\qquad \dot S=p,\qquad\\
\dot a_w&=-\tfrac{1}{\tau}a_w+\sqrt{q_c}\,w,\quad q_c=2\sigma_a^2/\tau .
\end{align}
\]

\subsection*{Assumptions}
\begin{itemize}[before=\small,itemsep=2pt,topsep=2pt,
  labelsep=0.6em,labelwidth=2.2em,labelindent=0pt,leftmargin=!]

\item[A1] \textbf{Bounded sampling and small linearization error.}
Sampling $h\in(0,\bar h]$ such that $\Phi(h,\tau)$ matches the analytic form; the
linearization error is Lipschitz on the compact set considered.

\item[A2] \textbf{Noise moments (nonpathological tails).}
Gyro, accelerometer, magnetometer, and pseudo-measurement noises have bounded second moments
(e.g., sub-Gaussian with scale $\bar\sigma$). The OU driver $w$ is standard white noise.

\item[A3] \textbf{Parameter compactness via projection.}
\[
\Theta=\{(\sigma_a,\tau):\underline\sigma\le\sigma_a\le\overline\sigma,\;
\underline\tau\le\tau\le\overline\tau\},
\]
enforced by a projection $\Pi_\Theta(\cdot)$.

\item[A4] \textbf{Rate-limited adaptation.} There exists $\rho>0$ with
$\|\theta_{k+1}-\theta_k\|\le\rho$ for all $k$ (per-step clamp).

\item[A5] \textbf{Smooth dependence.}
$\Phi(\theta)$ and $K(\theta)$ are Lipschitz on $\Theta$ with constants $L_\Phi,L_K$.

\item[A6] \textbf{CQLF for the frozen closed loop.}
Let $A(\theta):=\Phi(\theta)-K(\theta)C\,\Phi(\theta)$. There exist $P\succ0$ and $\mu\in(0,1)$ such that
\begin{equation}
\label{eq:cqlf}
A(\theta)^\top P\,A(\theta)\;\preceq\;(1-\mu)\,P\qquad \forall\,\theta\in\Theta .
\end{equation}

\item[A7] \textbf{Explicit adaptation law.}
$\theta_{k+1}=\Pi_\Theta\!\big(\theta_k-\Gamma\nabla_\theta\mathcal{L}(e_k,\theta_k)\big)$,
with $\Gamma=\mathrm{diag}(\gamma_\sigma,\gamma_\tau)$.

\item[A8] \textbf{Gradient regularity and small step.}
$\nabla_\theta\mathcal{L}$ is $L_\theta$-Lipschitz and $\Gamma$ is small enough that
$\|\theta_{k+1}-\theta_k\|\le\rho<\rho^\star$.

\item[A9] \textbf{(Optional, for parameter convergence) PE.}
Regressors driving $\nabla_\theta\mathcal{L}$ are persistently exciting on $\Theta$.
\end{itemize}

Let $e:=x-x^\star$ denote the stacked estimation error for the linear translational
subsystem (including $a_w$), with $x^\star$ the true OU-driven state.
Let $\tilde\theta:=\hat\theta-\theta^\star$ be parameter error relative to the
stationary values $(\sigma_a^\star,\tau^\star)$. Disturbances $d_k$ aggregate process/measurement noise,
linearization residue, and bounded coupling from attitude.

\begin{lemma}[Parameter-variation mismatch bound]\label{lem:delta}
Let $A(\theta)=\Phi(\theta)-K(\theta)C\Phi(\theta)$ and assume \textup{A4–A5}.
Then for $\Delta_k:=A(\theta_{k+1})-A(\theta_k)$,
\[
\fitcol{
\|\Delta_k\|\;\le\;\Big(L_\Phi+L_K\|C\|\,\|\Phi\|+\|K\|\|C\|\,L_\Phi\Big)\rho\;=:\;L_A\rho .
}
\]
\end{lemma}

\begin{proof}
Mean-value expansion over compact $\Theta$ and triangle inequality.
\end{proof}

\begin{lemma}[One-step Lyapunov decrement]\label{lem:onestep-corrected}
Under \textup{A1–A6}, with $V(e)=e^\top P e$ and $P,\mu$ as in \eqref{eq:cqlf}, the
time-varying update $e_{k+1}=A(\theta_k)e_k+\Delta_k e_k+d_k$ satisfies
\begin{equation}\label{eq:Vdec}
V(e_{k+1}) \;\le\; \big(1-\mu+c_1\|\Delta_k\|^2\big)V(e_k) + c_d\|d_k\|^2,
\end{equation}
where $c_1=\lambda_{\max}(P)/\lambda_{\min}(P)$ and $c_d>0$ depends on bounds of $A(\theta),P$ on $\Theta$.
\end{lemma}

\begin{proof}
Expand $V(e_{k+1})$, use \eqref{eq:cqlf}, Rayleigh bounds, and Young’s inequality to absorb linear
$\Delta_k$ terms and the cross term with $d_k$ into $\|\Delta_k\|^2V(e_k)$ and $\|d_k\|^2$.
\end{proof}

\begin{lemma}[Coupled parameter update bound]\label{lem:coupled}
Assume \textup{A7–A8}. Then
\[
\|\tilde\theta_{k+1}\|^2 \;\le\; (1-\eta)\,\|\tilde\theta_k\|^2 + c_\theta\,\|e_k\|^2,
\]
where $\eta>0$ depends on $\Gamma$ and cocoercivity of $\nabla_\theta\mathcal{L}$, and
$c_\theta$ on $L_\theta$.
\end{lemma}

\begin{proof}[Sketch]
Projection is nonexpansive; small-step gradient maps are averaged contractions. Expand
$\|\tilde\theta_{k+1}\|^2$ and apply Lipschitz/cocoercivity.
\end{proof}

\begin{theorem*}[ISS of the adaptive estimator (augmented state)]\label{thm:iss-aug}
Let $W_k := V(e_k)+\alpha\|\tilde\theta_k\|^2$ for some $\alpha>0$.
Under \textup{A1–A8} and $\rho<\rho^\star$,
there exist class-$\mathcal{KL}$ and class-$\mathcal{K}$ functions $\beta,\gamma$ such that
\begin{equation}\label{eq:issbound}
\|e_k\|+\|\tilde\theta_k\| \;\le\; \beta\!\big(\|e_0\|+\|\tilde\theta_0\|,k\big)
+ \gamma\!\Big(\sup_{0\le j<k}\|d_j\|\Big).
\end{equation}
If $d_k\to0$ then $e_k\to0$ and $\tilde\theta_k$ remains bounded. If, in addition, \textup{A9} holds,
then $\tilde\theta_k\to0$.
\end{theorem*}

\begin{proof}
Combine \eqref{eq:Vdec} with Lemma~\ref{lem:coupled}:
\[
\fitcol
W_{k+1} \le (1-\mu+c_1L_A^2\rho^2)V(e_k)
+ c_d\|d_k\|^2 + \alpha\big((1-\eta)\|\tilde\theta_k\|^2+c_\theta\|e_k\|^2\big).
}
\]
Choose $\alpha>0$ so the coefficient on $\|e_k\|^2$ is negative. Then
\[
W_{k+1}-W_k \le -\varepsilon V(e_k) - \alpha\eta\|\tilde\theta_k\|^2 + c_d\|d_k\|^2,
\]
an ISS Lyapunov inequality. The discrete ISS theorem yields \eqref{eq:issbound}. Under PE (A9),
standard adaptive arguments give $\tilde\theta_k\to0$.
\end{proof}

\paragraph{Interpretation.}
With small per-step parameter change $\rho$ and bounded-variance noise, $(e_k,\tilde\theta_k)$ is ISS;
$e_k\to0$ as disturbances vanish. The scaling $R_S=k_i\,\sigma_a\,\tau^3$ preserves uniform stability
\emph{provided} the frozen loop admits a CQLF on $\Theta$ (A6) and adaptation is slow (A4–A5, A8).

\subsection*{How to verify the CQLF (A6) in practice}
Discretize a grid of $(\sigma_a,\tau)$ in $\Theta$ and solve
\[
A(\theta_i)^\top P A(\theta_i) \preceq (1-\mu)P,\qquad P\succ0
\]
for a common $P,\mu$ using an LMI solver (e.g.\ \texttt{feasp}, \texttt{CVX}, or \texttt{YALMIP}/\texttt{MOSEK}).
If infeasible: (i) narrow $\Theta$; (ii) raise $R_S$ floors; (iii) reduce $\rho$; or (iv) allow a parameter-dependent
$P(\theta)$ (LPV).

\subsection*{Implementation checklist (A1–A9)}
\begin{enumerate}\itemsep2pt
\item Projection: $\hat\sigma_a\!\in[\underline\sigma,\overline\sigma]$, $\hat\tau\!\in[\underline\tau,\overline\tau]$.
\item Rate limit: clamp $\|\theta_{k+1}-\theta_k\|\!\le\!\rho<\rho^\star$.
\item Noise floors: keep $Q\succeq q_{\min}I$, $R,R_S\succeq r_{\min}I$.
\item Timescale separation: update $(\hat\sigma_a,\hat\tau)$ slower than $(v,p,S)$.
\item Anisotropy: $R_S=\mathrm{diag}(k_x,k_y,k_z)\,\sigma_a\tau^3$ with $k_x,k_y\!\ll\!k_z$ when appropriate.
\item (Optional) For $\tilde\theta_k\!\to\!0$: enforce PE (maneuvers or persistent spectral content).
\end{enumerate}

\section*{Appendix: Why $R_S \propto \sigma_a \tau^3$ (Dimensional and Spectral Justification)}
\label{app:rs-scaling}

\paragraph{Notation and units.}
$a_w$ is OU with std.\ $\sigma_a$ and correlation $\tau$,
\[
\fitcol{
\dot a_w = -\tfrac{1}{\tau}a_w + \sqrt{q_c}\,w, \qquad q_c=\tfrac{2\sigma_a^2}{\tau}.
}
\]
The chain $\dot v=a_w,\;\dot p=v,\;\dot S=p$ implies $S$ is the triple time integral of $a_w$.
$R_S$ is a standard deviation (so $R_S^2$ appears in covariance).

\begin{remark}[Back-of-the-envelope scaling]\label{rem:dim}
Over one correlation time $\tau$, each integration contributes a factor $\sim\tau$ in magnitude:
$v=\mathcal O(\sigma_a\tau)$, $p=\mathcal O(\sigma_a\tau^2)$, $S=\mathcal O(\sigma_a\tau^3)$.
This heuristic matches the spectral calculation below.
\end{remark}

\paragraph{Spectral justification with a low-frequency cutoff.}
With $S_a(\omega)=\tfrac{2\sigma_a^2\tau}{1+\omega^2\tau^2}$ and $H_S(j\omega)=1/(j\omega)^3$,
the variance of $S$ without a low-frequency cutoff diverges. Using a physically motivated cutoff
$\omega_{\min}\approx 1/\tau$ (or, equivalently, detrending over windows of length $\sim\tau$), we obtain
\[
\fitcol{
\mathrm{Var}[S]
= \frac{1}{2\pi}\!\int_{\omega_{\min}}^\infty \! S_a(\omega)\,\big|H_S(j\omega)\big|^2\,d\omega
\approx \frac{1}{2\pi}\!\int_{1/\tau}^\infty \frac{2\sigma_a^2\tau}{1+\omega^2\tau^2}\,\frac{1}{\omega^6}\,d\omega.
}
\]
With the substitution $u = \omega\tau$, $d\omega = du/\tau$, this becomes
\[
\fitcol{
\mathrm{Var}[S] \approx \frac{\sigma_a^2\tau^6}{\pi}\int_{1}^{\infty}\frac{du}{(1+u^2)\,u^6}.
}
\]
Numerically,
\[
\int_{1}^{\infty}\frac{du}{(1+u^2)\,u^6} \approx 0.0812686
\quad\Rightarrow\quad
\mathrm{Var}[S] \approx 0.0258686\,\sigma_a^2\tau^6,
\]
so
\[
\mathrm{std}(S)\approx 0.16084\,\sigma_a\,\tau^3.
\]
Thus choosing
\[
R_S = k_i\,\sigma_a\,\tau^3
\quad\Longleftrightarrow\quad
R_S^2 = k_i^2\,\sigma_a^2\,\tau^6
\]
makes the innovation SNR roughly invariant to sea-state energy and correlation length, consistent with
the OU chain’s discrete $Q_d$ scaling.
