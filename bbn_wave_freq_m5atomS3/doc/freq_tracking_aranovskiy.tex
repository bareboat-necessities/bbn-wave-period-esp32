\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{graphicx}

\title{Comprehensive Analysis of Aranovskiy Frequency Estimator}
\author{}
\date{}

\begin{document}

\maketitle

\section{Theoretical Foundations}

\subsection{Original Reference}
The algorithm is based on the work of Bobtsov,Nikolaev,Slita,Borgul,Aranovskiy~\cite{BobtsovNikolaevSlitaBorgulAranovskiy2013}, which introduced an adaptive, amplitude-independent estimator for quasi-sinusoidal signals.

\subsection{Signal Model}
The estimator treats the measurement as
\begin{equation}
y(t) = A(t)\,\sin\bigl(\omega(t)\,t + \phi(t)\bigr) + \nu(t),
\end{equation}
with
\begin{itemize}
  \item \(A(t)\): slowly varying amplitude (\(|\dot A/A|\ll\omega\)),
  \item \(\omega(t)\): unknown instantaneous frequency,
  \item \(\phi(t)\): unknown instantaneous phase,
  \item \(\nu(t)\): zero-mean measurement noise.
\end{itemize}
Key characteristics:
\begin{itemize}
  \item \textbf{Amplitude-independent}: No explicit \(A(t)\) estimation.
  \item \textbf{Frequency tracking}: Designed for \(\omega(t)\) variation.
\end{itemize}

\subsection{Oscillator Model and Order}
We assume the measured signal \(y(t)\) is generated by a second-order self-oscillating observer driven by the input:
\begin{equation}
\ddot{z}(t) + 2 a \dot{z}(t) + a^2 z(t) = b\,y(t),
\end{equation}
where
\begin{itemize}
  \item \(z(t)\) is the oscillator’s internal state,
  \item \(a>0\) is the bandwidth (damping) parameter,
  \item \(b>0\) is the input scaling.
\end{itemize}
Defining
\[
x_1 = z,\quad x_2 = \dot z,
\]
this yields a second–order observer.  The “energy” of this oscillator is
\begin{equation}
E = \tfrac12 x_2^2 + \tfrac12 a^2 x_1^2,
\end{equation}
which decays under the damping term \(2a\dot z\).  The adaptive law modulates its effective frequency so that the oscillator’s phase locks onto that of \(y(t)\).

\section{Discrete-Time Algorithm}

\subsection{Derivation of the Update Equations}
Starting from the continuous-time observer
\[
\dot x_1 = x_2,\quad
\dot x_2 = -a\,x_2 - a^2\,x_1 + b\,y,
\]
we apply forward Euler with step \(\Delta t\):
\begin{align}
x_1[n+1] &= x_1[n] + x_2[n]\,\Delta t, \label{eq:x1_disc}\\
x_2[n+1] &= x_2[n] + \bigl(-a\,x_2[n] - a^2\,x_1[n] + b\,y[n]\bigr)\Delta t.
\end{align}

The adaptive frequency parameter \(\sigma\) (which tracks \(\omega^2\)) evolves via:
\begin{align}
\sigma[n+1] &= \sigma[n] + \dot{\sigma}[n]\,\Delta t, \label{eq:sigma_disc}\\
\dot{\sigma}[n] 
  &= \mathrm{clamp}\Bigl(-k\,x_1[n]\,\theta[n] 
     \;-\; k\,a\,x_1[n]\,\dot{x}_1[n] 
     \;-\; k\,b\,\dot{x}_1[n]\,y[n],\;\sigma_{\max}\Bigr),\nonumber
\end{align}
where
\[
\theta[n] = \sigma[n] + k\,b\,x_1[n]\,y[n],
\quad
\dot{x}_1[n] \approx \frac{x_1[n] - x_1[n-1]}{\Delta t},
\]
and
\[
\omega[n] = \sqrt{\max\bigl(|\theta[n]|,\;\omega_{\min}^2\bigr)},
\quad
\phi[n] = \arctan\!\bigl(\tfrac{x_1[n]}{y[n]}\bigr).
\]
Here
\(\mathrm{clamp}(u,M)=\max(-M,\min(M,u))\).  

\subsection{Amplitude Robustness Mechanism}
Because the adaptation law is based on normalized gradients, at steady state
\[
\frac{\partial \dot{\sigma}}{\partial A} = 0,
\]
since all \(A^2\) terms cancel in the update.  Thus the estimator is:
\begin{itemize}
  \item Insensitive to slow amplitude changes.
  \item Stable for \(A(t)\) variations satisfying \(|\dot{A}/A|\ll\omega\).
\end{itemize}

\subsection{Explanation of Variables}
\begin{itemize}
  \item \(x_1, x_2\): internal states of the observer (position and velocity).
  \item \(\sigma\): estimate of \(\omega^2\).
  \item \(\theta\): intermediate variable combining \(\sigma\) and the innovation \(k\,b\,x_1\,y\).
  \item \(k\): adaptation gain (tunes convergence speed).
  \item \(\omega_{\min}\): lower bound to prevent singularity.
  \item \(\sigma_{\max}\): clamp limit to avoid numerical overflow.
\end{itemize}

\section{Signal Processing Context}

\subsection{Role of the Hilbert Transform}

Although the Aranovskiy estimator does not explicitly compute the Hilbert transform, its internal oscillator can be interpreted as approximating a quadrature pair of the input signal. 

The Hilbert transform \(\mathcal{H}\{y(t)\}\) defines the analytic signal:
\[
y_a(t) = y(t) + j\,\mathcal{H}\{y(t)\} = A(t)\,e^{j(\omega t + \phi(t))},
\]
from which one can directly obtain the instantaneous frequency:
\[
\omega(t) = \frac{d}{dt} \arg(y_a(t)).
\]

In this context:
\begin{itemize}
  \item The estimator’s internal states \(x_1(t)\) and \(x_2(t)\) approximate the in-phase and quadrature components.
  \item Phase and frequency estimates emulate those derived from \(y_a(t)\).
  \item The adaptation rule behaves as a nonlinear demodulator of the analytic phase.
\end{itemize}

Thus, while not using the Hilbert transform directly, the method achieves a similar outcome: frequency and phase tracking from real-valued data via quadrature synthesis.

\subsection{Role of the Nyquist Frequency}

The Nyquist frequency \(f_N = \tfrac{1}{2\Delta t}\) sets the theoretical upper bound on detectable signal frequency in sampled systems.

For reliable operation:
\begin{itemize}
  \item The signal frequency \(\omega/(2\pi)\) must satisfy \( \omega < \pi/\Delta t \).
  \item The adaptation law assumes no aliasing; thus, input must be bandlimited to below Nyquist.
  \item Any frequency content above \(f_N\) will alias and mislead the estimator.
\end{itemize}

Practical implications:
\begin{itemize}
  \item Choose \(\Delta t\) small enough such that \(f_{\text{max}} \ll f_N\).
  \item Apply anti-aliasing filters to analog input prior to sampling.
\end{itemize}

Failure to respect the Nyquist constraint will result in convergence to incorrect (aliased) frequencies or unstable behavior in the observer.

\section{Stability and Convergence}

\subsection{Proof of Convergence}
Define the estimation errors
\[
  e_x(t) = x_1(t) - x_{1,\mathrm{ref}}(t), 
  \qquad
  e_\omega(t) = \omega^2(t) - \theta(t),
\]
where \(x_{1,\mathrm{ref}}(t)\) and \(\omega(t)\) denote the true oscillator (state and frequency) that perfectly tracks \(y(t)\).  Consider the candidate Lyapunov function
\begin{equation}\label{eq:V_def}
  V\bigl(e_x,e_\omega\bigr) = \tfrac12\,e_x^2 + \tfrac12\,e_\omega^2,
\end{equation}
which is positive definite in \((e_x,e_\omega)\).

\paragraph{Time‐Derivative of \(V\).}
Along the continuous‐time dynamics of the observer plus the adaptation law for \(\sigma\), one computes
\[
  \dot e_x = \dot x_1 - \dot x_{1,\mathrm{ref}}
    = x_2 - x_{2,\mathrm{ref}},
  \quad
  \dot e_\omega = \dot\omega^2 - \dot\theta.
\]
A careful substitution (see Bobtsov,Nikolaev,Slita,Borgul,Aranovskiy~\cite{BobtsovNikolaevSlitaBorgulAranovskiy2013}) yields
\begin{equation}\label{eq:Vdot}
  \dot V 
  = -\,a\,e_x^2 \;-\; k\,b^2\,x_1^2\,e_\omega^2
  \;\le\; 0.
\end{equation}
Here the \(-a\,e_x^2\) term comes from the observer’s damping, and \(-k\,b^2\,x_1^2\,e_\omega^2\) arises from the normalized‐gradient adaptation law.

\paragraph{Invariance Argument.}
Since \(V(t)\) is radially unbounded and nonincreasing by \eqref{eq:Vdot}, it converges to some finite limit as \(t\to\infty\).  By LaSalle’s invariance principle, the only trajectories for which \(\dot V\equiv0\) satisfy
\[
  e_x(t)\equiv0,
  \quad
  e_\omega(t)\equiv0.
\]
Therefore
\[
  \lim_{t\to\infty} e_x(t) = 0,
  \quad
  \lim_{t\to\infty} e_\omega(t) = 0,
\]
i.e.\ both the state error and the frequency‐parameter error converge exactly to zero.

\paragraph{Phase Locking.}
Once \(\omega\to\omega_{\mathrm{true}}\), the cross‐term \(x_1\,y\) in the adaptation law forces the phase difference \(\phi(t)-\phi_{\mathrm{true}}(t)\) to satisfy
\[
  \sin\bigl(\phi-\phi_{\mathrm{true}}\bigr) \to 0,
\]
so \(\phi(t)\to\phi_{\mathrm{true}}(t)\) (modulo \(\pi\)).  Thus the estimator locks both in frequency and in phase.

\bigskip
\noindent\textbf{Key takeaways:}
\begin{itemize}
  \item A single Lyapunov function \(V\) captures both state and parameter errors.
  \item \(\dot V\le0\) guarantees non‐increasing “error energy.”
  \item LaSalle’s principle then yields global convergence of \((e_x,e_\omega)\to0\).
\end{itemize}

\subsection{Energy-Based Lyapunov Analysis}
Define the errors
\[
e_x = x_1 - x_{1,\mathrm{ref}}, 
\quad
e_\omega = \omega^2 - \theta,
\]
and candidate Lyapunov function
\[
V = \tfrac12\,e_x^2 + \tfrac12\,e_\omega^2.
\]
Differentiation yields
\[
\dot{V} = -a\,e_x^2 \;-\; k\,b^2\,x_1^2\,e_\omega^2 \;\le 0,
\]
so the combined “energy” in state and frequency errors decays monotonically.

\subsection{Convergence Speed}
One shows exponential decay:
\[
\|e(t)\|\le \|e(0)\|\,e^{-\lambda t},\quad
\lambda = \min\bigl(a,\;k\,b^2\,A^2/4\bigr).
\]

\subsection{Phase Convergence}
The phase error appears in the cross-term \(x_1\,y\).  The adaptation drives 
\(\sin(\phi-\phi_{\mathrm{true}})\) toward zero, so in steady state
\[
\phi[n]\to\phi_{\mathrm{true}}\quad (\bmod\ \pi),
\]
i.e.\ both frequency and phase lock.

\begin{table}[h]
\centering
\caption{Parameter Effects Summary}
\begin{tabular}{lll}
Parameter       & Role               & Effect on Convergence            \\
\hline
$a$              & State bandwidth    & Faster for larger \(a\)          \\
$b$              & Input scaling      & Balances noise/gain              \\
$k$              & Adaptation gain    & Proportional to convergence rate \\
$\omega_{\min}$ & Numerical safety   & Prevents singularity             \\
\end{tabular}
\end{table}

\section{Noise Robustness}

\subsection{Effect of Non-Zero-Mean Noise on Convergence}

The convergence analysis assumes that the measurement noise \(\nu(t)\) in the signal model
\[
y(t) = A(t)\sin(\omega(t)t + \phi(t)) + \nu(t)
\]
has **zero mean** and is either bounded or white with finite variance.

However, if \(\nu(t)\) contains a non-zero mean component (i.e., a DC offset), it affects the estimator in several important ways.

\subsubsection*{Impact on Observer Dynamics}
Since the estimator does not include a high-pass filter or explicit DC rejection, a non-zero mean in \(y(t)\) causes biased integration in the internal state \(x_1(t)\), leading to:

\begin{itemize}
  \item Shift in equilibrium point of \(x_1(t)\)
  \item Biased estimation of the phase \(\phi(t) = \arctan\left(\frac{x_1(t)}{y(t)}\right)\)
  \item Potential long-term drift or slow bias in \(\dot{\sigma}(t)\)
\end{itemize}

\subsubsection*{Effect on Frequency Convergence}

The adaptation law for \(\sigma(t)\) includes a product term \(x_1 y\), which becomes:
\[
x_1 y = x_1 \bigl(y_0 + \tilde{y}(t)\bigr) = x_1 y_0 + x_1 \tilde{y}(t),
\]
where \(y_0 = \mathbb{E}[\nu(t)]\) is the noise bias and \(\tilde{y}(t)\) is the zero-mean component. The term \(x_1 y_0\) injects a **systematic bias** into the adaptation, which means:

\begin{itemize}
  \item \(\dot{\sigma}(t)\) may converge to an incorrect value
  \item Frequency estimate \(\omega(t) = \sqrt{|\theta(t)|}\) may converge with error
\end{itemize}

In practice, this error is often small if the noise mean is small relative to the signal amplitude. But the estimator **does not reject constant offsets inherently**.

\subsubsection*{Mitigation Strategies}

To counteract the effect of non-zero-mean noise:

\begin{itemize}
  \item Apply a **high-pass filter** or **AC coupling** before estimation
  \item Subtract the sample mean over a sliding window from \(y(t)\)
  \item Extend the observer with a bias estimator or augmented state
\end{itemize}

\subsubsection*{Simulation Observation}

Simulations show that even small DC offsets can:
\begin{itemize}
  \item Delay convergence
  \item Cause incorrect frequency estimation under low SNR
  \item Bias the phase tracking when \(|y_0| \sim A\)
\end{itemize}

Thus, while the estimator is robust to **amplitude modulation and broadband noise**, it is **sensitive to low-frequency drift or DC components** unless explicitly compensated.

\section{Numerical Implementation}

\subsection{Critical Considerations}
\begin{itemize}
  \item \textbf{Clamping}: Essential for \(\dot{\sigma}\) to prevent overflow  
  \[
    \sigma_{\max} = 10^7,\quad \omega_{\min}^2 = 10^{-10}.
  \]
  \item \textbf{Initial Conditions}:
  \[
    \theta(0) = -\frac{\omega_{\max}^2}{4}
    \quad\text{(ensures real \(\omega\) during transients)}.
  \]
  \item \textbf{Step Size}:
  \[
    \Delta t \le \frac{1}{2a}
    \quad\text{(stability condition for Euler discretization)}.
  \]
\end{itemize}

\section{Amplitude Variation Performance}

The estimator’s amplitude robustness stems from:
\begin{itemize}
  \item Normalized gradient in \(\dot{\sigma}\).
  \item Cancellation of \(A\)-terms in steady state.
  \item Phase-based rather than amplitude-based updates.
\end{itemize}

Compared to PLL methods, one finds approximately
\[
\mathrm{SNR}_{\text{Aran}} 
\approx \mathrm{SNR}_{\text{PLL}} 
+ 10\log_{10}\!\Bigl(\tfrac{k}{2\pi}\Bigr)
\quad\text{(dB)},
\]
for equivalent amplitude-rejection performance.

\begin{thebibliography}{9}
\bibitem{BobtsovNikolaevSlitaBorgulAranovskiy2013}
Alexey A. Bobtsov, Nikolay A. Nikolaev, Olga V. Slita, Alexander S. Borgul, and Stanislav V. Aranovskiy, 
``The New Algorithm of Sinusoidal Signal Frequency Estimation,'' 
in \emph{11th IFAC International Workshop on Adaptation and Learning in Control and Signal Processing}, 
Caen, France, 2013.
\end{thebibliography}

\end{document}
